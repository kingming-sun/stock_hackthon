## 1. æ¶æ„è®¾è®¡

### 1.1 åŒé“¾è·¯æ¶æ„æ¦‚è§ˆ
```mermaid
graph TD
    A[ç”¨æˆ·æµè§ˆå™¨] --> B[Reactå‰ç«¯åº”ç”¨]
    B --> C[FastAPIåç«¯æœåŠ¡]
    
    C --> D[ç›´æ¥APIé“¾è·¯]
    C --> E[Agentæ™ºèƒ½åˆ†æé“¾è·¯]
    
    D --> F[Redisç¼“å­˜]
    D --> G[Alpha Vantage API]
    
    E --> H[LangGraph Agent]
    H --> I[MCPå·¥å…·é›†]
    I --> G
    
    H --> J[LLMæœåŠ¡]
    
    F --> D
    
    subgraph "å‰ç«¯å±‚"
        B
    end
    
    subgraph "åç«¯æœåŠ¡å±‚"
        C
        D
        E
    end
    
    subgraph "æ•°æ®é“¾è·¯å±‚"
        F
        G
    end
    
    subgraph "AIæ™ºèƒ½å±‚"
        H
        I
        J
    end
```

### 1.2 åŒé“¾è·¯è¯¦ç»†è®¾è®¡

**ç›´æ¥APIé“¾è·¯ï¼ˆé«˜é€Ÿé€šé“ï¼‰**:
- ç”¨é€”ï¼šå¿«é€Ÿè·å–åŸºç¡€è‚¡ç¥¨æ•°æ®ï¼Œä½å»¶è¿Ÿå“åº”
- è·¯å¾„ï¼šPythonåç«¯ â†’ Redisç¼“å­˜ â†’ Alpha Vantage API
- ç‰¹ç‚¹ï¼šç®€å•ç›´æ¥ã€é«˜æ€§èƒ½ã€é€‚åˆå®æ—¶æ•°æ®è·å–

**Agentæ™ºèƒ½åˆ†æé“¾è·¯ï¼ˆæ™ºèƒ½é€šé“ï¼‰**:
- ç”¨é€”ï¼šå¤æ‚å¤šæ­¥éª¤åˆ†æï¼Œç»“åˆå¤šç»´åº¦æ•°æ®
- è·¯å¾„ï¼šPythonåç«¯ â†’ LangGraph Agent â†’ MCPå·¥å…· â†’ Alpha Vantage API â†’ LLMåˆ†æ
- ç‰¹ç‚¹ï¼šæ™ºèƒ½åŒ–ã€å¯ç¼–æ’ã€é€‚åˆæ·±åº¦åˆ†æ

## 2. æŠ€æœ¯æ ˆæè¿°

- **å‰ç«¯**: React@18 + TypeScript@5 + Tailwind CSS@3 + Vite
- **åç«¯**: FastAPI@0.104 + Python@3.11
- **ç¼“å­˜**: Redis@7
- **AIæ¡†æ¶**: LangGraph + LangChain
- **APIè°ƒç”¨**: httpx@0.25
- **æ•°æ®æ ¼å¼**: Pandas@2 + NumPy@1.24
- **éƒ¨ç½²**: Docker + Uvicorn

## 3. è·¯ç”±å®šä¹‰

### 3.1 ç›´æ¥APIé“¾è·¯ç«¯ç‚¹ï¼ˆé«˜é€Ÿé€šé“ï¼‰
| è·¯ç”± | ç”¨é€” | å“åº”æ—¶é—´ |
|-------|---------|----------|
| /api/stock/{symbol}/data | è·å–è‚¡ç¥¨å®æ—¶æ•°æ® | < 200ms |
| /api/stock/{symbol}/technical | è·å–æŠ€æœ¯åˆ†ææŒ‡æ ‡ | < 300ms |
| /api/stock/{symbol}/price | è·å–å½“å‰ä»·æ ¼ | < 100ms |
| /api/stock/{symbol}/history | è·å–å†å²æ•°æ® | < 500ms |

### 3.2 Agentæ™ºèƒ½åˆ†æé“¾è·¯ç«¯ç‚¹ï¼ˆæ™ºèƒ½é€šé“ï¼‰
| è·¯ç”± | ç”¨é€” | å¤„ç†æ—¶é—´ |
|-------|---------|----------|
| /api/stock/{symbol}/analysis | ç»¼åˆæ™ºèƒ½åˆ†æ | 5-15ç§’ |
| /api/stock/{symbol}/strategy | æŠ•èµ„ç­–ç•¥å»ºè®® | 8-20ç§’ |
| /api/stock/{symbol}/report | ç”ŸæˆæŠ•èµ„æŠ¥å‘Š | 10-30ç§’ |
| /api/analysis/status/{task_id} | æŸ¥è¯¢åˆ†æä»»åŠ¡çŠ¶æ€ | < 100ms |

### 3.3 ç³»ç»Ÿç®¡ç†ç«¯ç‚¹
| è·¯ç”± | ç”¨é€” |
|-------|---------|
| /api/cache/clear | æ¸…ç†ç¼“å­˜ |
| /api/system/status | ç³»ç»ŸçŠ¶æ€ç›‘æ§ |
| /health | å¥åº·æ£€æŸ¥ |

## 4. åŒé“¾è·¯æ ¸å¿ƒå®ç°

### 4.1 é“¾è·¯é€‰æ‹©ç­–ç•¥
```python
class LinkageRouter:
    """åŒé“¾è·¯è·¯ç”±å™¨"""
    
    def __init__(self):
        self.direct_link = DirectAPILink()
        self.agent_link = AgentAnalysisLink()
    
    async def route_request(self, symbol: str, request_type: str, params: dict) -> dict:
        """æ ¹æ®è¯·æ±‚ç±»å‹æ™ºèƒ½é€‰æ‹©é“¾è·¯"""
        
        # é«˜é€Ÿæ•°æ®è·å– - ä½¿ç”¨ç›´æ¥APIé“¾è·¯
        if request_type in ['price', 'quote', 'intraday']:
            return await self.direct_link.get_real_time_data(symbol, params)
        
        # å¤æ‚åˆ†æ - ä½¿ç”¨Agentæ™ºèƒ½é“¾è·¯
        elif request_type in ['analysis', 'strategy', 'report']:
            return await self.agent_link.get_intelligent_analysis(symbol, params)
        
        # æŠ€æœ¯æŒ‡æ ‡ - æ ¹æ®å¤æ‚åº¦é€‰æ‹©
        elif request_type == 'technical':
            indicators = params.get('indicators', [])
            if len(indicators) <= 3:  # ç®€å•æŒ‡æ ‡ï¼Œç”¨ç›´æ¥é“¾è·¯
                return await self.direct_link.get_technical_indicators(symbol, params)
            else:  # å¤æ‚æŒ‡æ ‡ç»„åˆï¼Œç”¨Agenté“¾è·¯
                return await self.agent_link.get_advanced_analysis(symbol, params)
        
        # é»˜è®¤ä½¿ç”¨ç›´æ¥é“¾è·¯
        else:
            return await self.direct_link.get_basic_data(symbol, params)
```

### 4.2 ç›´æ¥APIé“¾è·¯ï¼ˆé«˜é€Ÿé€šé“ï¼‰
```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant FastAPI
    participant Redis
    participant AlphaVantage

    User->>Frontend: è¯·æ±‚è‚¡ç¥¨æ•°æ®
    Frontend->>FastAPI: GET /api/stock/AAPL/data
    FastAPI->>Redis: æ£€æŸ¥ç¼“å­˜
    alt ç¼“å­˜å‘½ä¸­
        Redis-->>FastAPI: è¿”å›ç¼“å­˜æ•°æ®
    else ç¼“å­˜æœªå‘½ä¸­
        FastAPI->>AlphaVantage: è°ƒç”¨API
        AlphaVantage-->>FastAPI: è¿”å›å®æ—¶æ•°æ®
        FastAPI->>Redis: ç¼“å­˜æ•°æ®(5åˆ†é’ŸTTL)
    end
    FastAPI-->>Frontend: è¿”å›æ•°æ®
    Frontend-->>User: æ˜¾ç¤ºç»“æœ

### 4.5 Agentæ™ºèƒ½é“¾è·¯æ ¸å¿ƒå®ç°
```python
class AgentAnalysisLink:
    """Agentæ™ºèƒ½åˆ†æé“¾è·¯ - æ™ºèƒ½é€šé“"""
    
    def __init__(self):
        self.langgraph_agent = StockAnalysisAgent()
        self.task_manager = TaskManager()
        self.mcp_tools = AlphaVantageMCP()
    
    async def get_intelligent_analysis(self, symbol: str, params: dict) -> dict:
        """è·å–æ™ºèƒ½åˆ†æ - ç›®æ ‡å¤„ç†æ—¶é—´ 5-15ç§’"""
        
        # 1. åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        task = self.task_manager.create_task(symbol, "comprehensive_analysis")
        
        # 2. å¯åŠ¨LangGraphå·¥ä½œæµ
        workflow_input = {
            "symbol": symbol,
            "analysis_type": params.get("type", "comprehensive"),
            "include_news": params.get("include_news", True),
            "time_horizon": params.get("time_horizon", "medium_term"),
            "task_id": task.id
        }
        
        # 3. å¼‚æ­¥æ‰§è¡Œåˆ†ææµç¨‹
        asyncio.create_task(self._run_analysis_workflow(workflow_input))
        
        # 4. ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œå‰ç«¯å¯ä»¥è½®è¯¢çŠ¶æ€
        return {
            "task_id": task.id,
            "status": "processing",
            "estimated_time": "5-15 seconds",
            "linkage_type": "agent",
            "query_url": f"/api/analysis/status/{task.id}"
        }
    
    async def _run_analysis_workflow(self, input_data: dict):
        """è¿è¡ŒLangGraphåˆ†æå·¥ä½œæµ"""
        try:
            task_id = input_data["task_id"]
            symbol = input_data["symbol"]
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.task_manager.update_task_status(task_id, "running")
            
            # æ„å»ºLangGraphçŠ¶æ€
            state = AnalysisState(
                symbol=symbol,
                analysis_type=input_data["analysis_type"],
                current_step="init"
            )
            
            # æ‰§è¡Œå·¥ä½œæµèŠ‚ç‚¹
            result = await self.langgraph_agent.run_workflow(state)
            
            # æ›´æ–°ä»»åŠ¡å®ŒæˆçŠ¶æ€
            self.task_manager.update_task_status(
                task_id, 
                "completed", 
                result=result
            )
            
        except Exception as e:
            # æ›´æ–°ä»»åŠ¡å¤±è´¥çŠ¶æ€
            self.task_manager.update_task_status(
                task_id, 
                "failed", 
                error=str(e)
            )

class StockAnalysisAgent:
    """LangGraphè‚¡ç¥¨åˆ†æAgent"""
    
    def __init__(self):
        self.workflow = self._build_analysis_workflow()
        self.mcp_tools = AlphaVantageMCP()
        self.llm = ChatOpenAI(model="gpt-4-turbo-preview")
    
    def _build_analysis_workflow(self) -> StateGraph:
        """æ„å»ºåˆ†æå·¥ä½œæµ"""
        
        workflow = StateGraph(AnalysisState)
        
        # å®šä¹‰å·¥ä½œæµèŠ‚ç‚¹
        workflow.add_node("fetch_basic_data", self._fetch_basic_data)
        workflow.add_node("technical_analysis", self._technical_analysis)
        workflow.add_node("news_sentiment_analysis", self._news_sentiment_analysis)
        workflow.add_node("fundamental_analysis", self._fundamental_analysis)
        workflow.add_node("comprehensive_reasoning", self._comprehensive_reasoning)
        workflow.add_node("generate_recommendation", self._generate_recommendation)
        
        # å®šä¹‰æ¡ä»¶è¾¹
        workflow.add_edge("fetch_basic_data", "technical_analysis")
        workflow.add_edge("technical_analysis", "news_sentiment_analysis")
        workflow.add_edge("news_sentiment_analysis", "fundamental_analysis")
        workflow.add_edge("fundamental_analysis", "comprehensive_reasoning")
        workflow.add_edge("comprehensive_reasoning", "generate_recommendation")
        
        workflow.set_entry_point("fetch_basic_data")
        workflow.set_finish_point("generate_recommendation")
        
        return workflow.compile()
    
    async def _fetch_basic_data(self, state: AnalysisState) -> AnalysisState:
        """è·å–åŸºç¡€æ•°æ® - ä½¿ç”¨MCPå·¥å…·"""
        
        # å¹¶è¡Œè·å–å¤šç§æ•°æ®
        tasks = [
            self.mcp_tools.get_stock_quote(state["symbol"]),
            self.mcp_tools.get_company_overview(state["symbol"]),
            self.mcp_tools.get_latest_news(state["symbol"])
        ]
        
        quote_data, company_data, news_data = await asyncio.gather(*tasks)
        
        # æ›´æ–°çŠ¶æ€
        state["stock_data"] = {
            "quote": quote_data,
            "company": company_data,
            "news": news_data[:5]  # åªå–å‰5æ¡æ–°é—»
        }
        state["current_step"] = "basic_data_fetched"
        
        return state
    
    async def _technical_analysis(self, state: AnalysisState) -> AnalysisState:
        """æŠ€æœ¯åˆ†æ - å¤šæŒ‡æ ‡ç»¼åˆ"""
        
        symbol = state["symbol"]
        
        # è·å–æŠ€æœ¯æŒ‡æ ‡
        indicators = await self.mcp_tools.get_multiple_indicators(
            symbol,
            indicators=["SMA", "EMA", "RSI", "MACD", "BBANDS", "STOCH"]
        )
        
        # LLMåˆ†ææŠ€æœ¯å½¢æ€
        technical_prompt = f"""
        åŸºäºä»¥ä¸‹æŠ€æœ¯æŒ‡æ ‡ï¼Œåˆ†æ{symbol}çš„æŠ€æœ¯å½¢æ€ï¼š
        {json.dumps(indicators, indent=2)}
        
        è¯·æä¾›ï¼š
        1. è¶‹åŠ¿åˆ¤æ–­ï¼ˆbullish/bearish/neutralï¼‰
        2. å…³é”®æ”¯æ’‘é˜»åŠ›ä½
        3. ä¹°å…¥å–å‡ºä¿¡å·
        4. é£é™©ç­‰çº§ï¼ˆ1-10ï¼‰
        """
        
        technical_analysis = await self.llm.ainvoke(technical_prompt)
        
        state["technical_data"] = {
            "indicators": indicators,
            "analysis": technical_analysis.content,
            "signals": self._parse_technical_signals(technical_analysis.content)
        }
        state["current_step"] = "technical_analysis_completed"
        
        return state
```
```

### 4.3 ç›´æ¥é“¾è·¯æ€§èƒ½ä¼˜åŒ–
```python
class DirectAPILink:
    """ç›´æ¥APIé“¾è·¯ - é«˜é€Ÿé€šé“"""
    
    def __init__(self):
        self.cache = RedisCache()
        self.alpha_client = AlphaVantageClient()
        self.circuit_breaker = CircuitBreaker()
    
    async def get_real_time_data(self, symbol: str, params: dict) -> dict:
        """è·å–å®æ—¶è‚¡ç¥¨æ•°æ® - ç›®æ ‡å“åº”æ—¶é—´ < 200ms"""
        
        cache_key = f"direct:realtime:{symbol}"
        
        # 1. ç¼“å­˜æŸ¥è¯¢ï¼ˆ~10msï¼‰
        cached_data = await self.cache.get(cache_key)
        if cached_data and self._is_cache_valid(cached_data):
            return {
                **cached_data,
                "cache_hit": True,
                "response_time": "< 50ms",
                "linkage_type": "direct"
            }
        
        # 2. ç†”æ–­å™¨ä¿æŠ¤
        if not self.circuit_breaker.can_execute():
            return await self._get_fallback_data(symbol)
        
        try:
            # 3. å¹¶è¡ŒAPIè°ƒç”¨ï¼ˆ~150msï¼‰
            tasks = [
                self.alpha_client.get_quote(symbol),
                self.alpha_client.get_volume(symbol),
                self.alpha_client.get_market_cap(symbol)
            ]
            
            quote, volume, market_cap = await asyncio.gather(*tasks)
            
            # 4. æ•°æ®èšåˆ
            result = {
                "symbol": symbol,
                "price": quote["price"],
                "change": quote["change"],
                "change_percent": quote["change_percent"],
                "volume": volume,
                "market_cap": market_cap,
                "timestamp": datetime.now().isoformat(),
                "cache_hit": False,
                "response_time": "~150ms",
                "linkage_type": "direct"
            }
            
            # 5. å¼‚æ­¥ç¼“å­˜æ›´æ–°
            await self.cache.set(cache_key, result, ttl=60)  # 1åˆ†é’Ÿç¼“å­˜
            
            return result
            
        except Exception as e:
            self.circuit_breaker.record_failure()
            return await self._get_fallback_data(symbol)
    
    def _is_cache_valid(self, cached_data: dict) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ•°æ®æ˜¯å¦ä»ç„¶æœ‰æ•ˆ"""
        timestamp = datetime.fromisoformat(cached_data["timestamp"])
        return datetime.now() - timestamp < timedelta(minutes=1)
    
    async def _get_fallback_data(self, symbol: str) -> dict:
        """é™çº§å¤„ç† - è¿”å›è¿‡æœŸç¼“å­˜æˆ–åŸºç¡€æ•°æ®"""
        # å®ç°é™çº§é€»è¾‘
        pass
```

### 4.2 Agentæ™ºèƒ½åˆ†æé“¾è·¯
```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant FastAPI
    participant LangGraph
    participant MCP
    participant AlphaVantage
    participant AI

    User->>Frontend: è¯·æ±‚AIåˆ†æ
    Frontend->>FastAPI: POST /api/stock/AAPL/analysis
    FastAPI->>LangGraph: åˆ›å»ºåˆ†æä»»åŠ¡
    LangGraph->>MCP: è°ƒç”¨æ•°æ®è·å–å·¥å…·
    MCP->>AlphaVantage: è·å–è‚¡ç¥¨æ•°æ®
    AlphaVantage-->>MCP: è¿”å›æ•°æ®
    MCP-->>LangGraph: è¿”å›å¤„ç†ç»“æœ
    LangGraph->>AI: è¯·æ±‚AIåˆ†æ
    AI-->>LangGraph: è¿”å›åˆ†æç»“æœ
    LangGraph-->>FastAPI: è¿”å›å®Œæ•´åˆ†æ
    FastAPI-->>Frontend: è¿”å›åˆ†ææŠ¥å‘Š
```

## 5. ç¼“å­˜ç­–ç•¥

### 5.1 ç¼“å­˜é”®è®¾è®¡
```python
# è‚¡ç¥¨æ•°æ®ç¼“å­˜é”®
STOCK_DATA_KEY = f"stock:{symbol}:data"
TECHNICAL_KEY = f"stock:{symbol}:technical:{indicator}"
NEWS_KEY = f"stock:{symbol}:news"
ANALYSIS_KEY = f"stock:{symbol}:analysis:{analysis_type}"
```

### 5.2 TTLè®¾ç½®
```python
CACHE_TTL = {
    'stock_data': 300,      # 5åˆ†é’Ÿ
    'technical': 600,       # 10åˆ†é’Ÿ
    'news': 1800,          # 30åˆ†é’Ÿ
    'analysis': 3600,       # 1å°æ—¶
}
```

## 6. APIæ¥å£è®¾è®¡

### 6.1 è‚¡ç¥¨æ•°æ®API
```python
# è·å–è‚¡ç¥¨å®æ—¶æ•°æ®
GET /api/stock/{symbol}/data
Response: {
    "symbol": "AAPL",
    "price": 175.43,
    "change": 2.15,
    "change_percent": 1.24,
    "volume": 45678900,
    "market_cap": 2800000000000,
    "timestamp": "2024-01-15T15:30:00Z",
    "cache_hit": false
}
```

### 6.2 æŠ€æœ¯åˆ†æAPI
```python
# è·å–æŠ€æœ¯åˆ†ææŒ‡æ ‡
GET /api/stock/{symbol}/technical?indicators=sma,rsi,macd
Response: {
    "symbol": "AAPL",
    "indicators": {
        "sma_20": 172.45,
        "sma_50": 168.92,
        "rsi": 65.3,
        "macd": {
            "value": 2.15,
            "signal": 1.87,
            "histogram": 0.28
        }
    },
    "signals": {
        "trend": "bullish",
        "momentum": "strong",
        "volatility": "moderate"
    }
}
```

### 6.3 AIåˆ†æAPI
```python
# AIæ™ºèƒ½åˆ†æ
POST /api/stock/{symbol}/analysis
Request: {
    "analysis_type": "comprehensive",
    "include_news": true,
    "time_horizon": "medium_term"
}

Response: {
    "symbol": "AAPL",
    "analysis": {
        "technical_summary": "æŠ€æœ¯é¢æ˜¾ç¤ºå¼ºåŠ²ä¸Šå‡è¶‹åŠ¿...",
        "fundamental_summary": "åŸºæœ¬é¢ç¨³å¥ï¼Œä¼°å€¼åˆç†...",
        "news_sentiment": "è¿‘æœŸæ–°é—»åæ­£é¢...",
        "risk_assessment": "ä¸­ç­‰é£é™©æ°´å¹³...",
        "recommendation": "å»ºè®®æŒæœ‰",
        "confidence_score": 0.78,
        "key_factors": [
            "RSIæŒ‡æ ‡æ˜¾ç¤ºè¶…ä¹°",
            "æˆäº¤é‡æ”¾å¤§",
            "çªç ´å…³é”®é˜»åŠ›ä½"
        ]
    },
    "generated_at": "2024-01-15T15:35:00Z"
}
```

## 7. LangGraph Agentè®¾è®¡

### 7.1 AgentçŠ¶æ€å®šä¹‰
```python
from typing import TypedDict, List
from langgraph.graph import StateGraph

class AnalysisState(TypedDict):
    symbol: str
    stock_data: dict
    technical_data: dict
    news_data: List[dict]
    analysis_result: dict
    current_step: str

class StockAnalysisAgent:
    def __init__(self):
        self.workflow = StateGraph(AnalysisState)
        self._setup_workflow()
    
    def _setup_workflow(self):
        # å®šä¹‰å·¥ä½œæµèŠ‚ç‚¹
        self.workflow.add_node("fetch_data", self._fetch_stock_data)
        self.workflow.add_node("technical_analysis", self._analyze_technical)
        self.workflow.add_node("news_analysis", self._analyze_news)
        self.workflow.add_node("comprehensive_analysis", self._comprehensive_analysis)
        
        # å®šä¹‰å·¥ä½œæµè¾¹
        self.workflow.add_edge("fetch_data", "technical_analysis")
        self.workflow.add_edge("technical_analysis", "news_analysis")
        self.workflow.add_edge("news_analysis", "comprehensive_analysis")
        self.workflow.set_entry_point("fetch_data")
```

### 7.2 MCPå·¥å…·é›†æˆ
```python
from mcp import Client

class AlphaVantageTools:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://www.alphavantage.co/query"
    
    async def get_stock_data(self, symbol: str) -> dict:
        """è·å–è‚¡ç¥¨å®æ—¶æ•°æ®"""
        params = {
            "function": "GLOBAL_QUOTE",
            "symbol": symbol,
            "apikey": self.api_key
        }
        async with httpx.AsyncClient() as client:
            response = await client.get(self.base_url, params=params)
            return response.json()
    
    async def get_technical_indicator(self, symbol: str, indicator: str) -> dict:
        """è·å–æŠ€æœ¯æŒ‡æ ‡"""
        params = {
            "function": indicator,
            "symbol": symbol,
            "interval": "daily",
            "time_period": "20",
            "series_type": "close",
            "apikey": self.api_key
        }
        async with httpx.AsyncClient() as client:
            response = await client.get(self.base_url, params=params)
            return response.json()
    
    async def get_news(self, symbol: str) -> List[dict]:
        """è·å–ç›¸å…³æ–°é—»"""
        params = {
            "function": "NEWS_SENTIMENT",
            "tickers": symbol,
            "apikey": self.api_key
        }
        async with httpx.AsyncClient() as client:
            response = await client.get(self.base_url, params=params)
            return response.json().get("feed", [])
```

## 8. å†…å­˜çŠ¶æ€ç®¡ç†

### 8.1 ä¼šè¯çŠ¶æ€
```python
from typing import Dict, Any
import asyncio
from datetime import datetime, timedelta

class SessionManager:
    def __init__(self):
        self.sessions: Dict[str, Dict[str, Any]] = {}
        self.lock = asyncio.Lock()
    
    async def create_session(self, session_id: str) -> Dict[str, Any]:
        async with self.lock:
            self.sessions[session_id] = {
                "created_at": datetime.now(),
                "last_accessed": datetime.now(),
                "data": {}
            }
            return self.sessions[session_id]
    
    async def get_session(self, session_id: str) -> Dict[str, Any]:
        async with self.lock:
            session = self.sessions.get(session_id)
            if session:
                session["last_accessed"] = datetime.now()
                return session
            return None
    
    async def cleanup_expired_sessions(self):
        """æ¸…ç†è¿‡æœŸä¼šè¯"""
        async with self.lock:
            now = datetime.now()
            expired = []
            for session_id, session in self.sessions.items():
                if now - session["last_accessed"] > timedelta(hours=2):
                    expired.append(session_id)
            
            for session_id in expired:
                del self.sessions[session_id]
```

### 8.2 åˆ†æä»»åŠ¡çŠ¶æ€
```python
from enum import Enum
from typing import Optional
import uuid

class AnalysisStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

class AnalysisTask:
    def __init__(self, symbol: str, analysis_type: str):
        self.id = str(uuid.uuid4())
        self.symbol = symbol
        self.analysis_type = analysis_type
        self.status = AnalysisStatus.PENDING
        self.result: Optional[dict] = None
        self.error: Optional[str] = None
        self.created_at = datetime.now()
        self.updated_at = datetime.now()

class TaskManager:
    def __init__(self):
        self.tasks: Dict[str, AnalysisTask] = {}
    
    def create_task(self, symbol: str, analysis_type: str) -> AnalysisTask:
        task = AnalysisTask(symbol, analysis_type)
        self.tasks[task.id] = task
        return task
    
    def get_task(self, task_id: str) -> Optional[AnalysisTask]:
        return self.tasks.get(task_id)
    
    def update_task_status(self, task_id: str, status: AnalysisStatus, result: dict = None, error: str = None):
        if task_id in self.tasks:
            task = self.tasks[task_id]
            task.status = status
            task.result = result
            task.error = error
            task.updated_at = datetime.now()
```

## 9. é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶

### 9.1 APIè°ƒç”¨é‡è¯•
```python
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential

class APIClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://www.alphavantage.co/query"
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def fetch_with_retry(self, params: dict) -> dict:
        """å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨"""
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(self.base_url, params=params)
            
            if response.status_code == 429:  # é€Ÿç‡é™åˆ¶
                raise Exception("Rate limit exceeded")
            
            response.raise_for_status()
            data = response.json()
            
            # æ£€æŸ¥APIé”™è¯¯å“åº”
            if "Error Message" in data:
                raise Exception(f"API Error: {data['Error Message']}")
            
            if "Note" in data:  # APIé¢‘ç‡é™åˆ¶æç¤º
                raise Exception(f"API Limit: {data['Note']}")
            
            return data
```

### 9.2 é™çº§å¤„ç†
```python
class FallbackManager:
    def __init__(self):
        self.fallback_data = {}
    
    async def get_stock_data_with_fallback(self, symbol: str) -> dict:
        """è·å–è‚¡ç¥¨æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›ç¼“å­˜çš„é™çº§æ•°æ®"""
        try:
            # å°è¯•è·å–å®æ—¶æ•°æ®
            data = await self.fetch_real_time_data(symbol)
            # æ›´æ–°é™çº§ç¼“å­˜
            self.fallback_data[symbol] = data
            return data
        except Exception as e:
            # ä½¿ç”¨é™çº§æ•°æ®
            if symbol in self.fallback_data:
                return {
                    **self.fallback_data[symbol],
                    "warning": "Using cached data due to API failure",
                    "last_updated": self.fallback_data[symbol].get("timestamp")
                }
            else:
                raise Exception(f"No data available for {symbol}")
```

### 4.6 MCPå·¥å…·é›†å®ç°
```python
class AlphaVantageMCP:
    """Alpha Vantage MCPå·¥å…·é›†"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://www.alphavantage.co/query"
        self.rate_limiter = RateLimiter(calls_per_minute=5)
    
    async def get_stock_quote(self, symbol: str) -> dict:
        """è·å–è‚¡ç¥¨å®æ—¶æŠ¥ä»·"""
        async with self.rate_limiter:
            params = {
                "function": "GLOBAL_QUOTE",
                "symbol": symbol,
                "apikey": self.api_key
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.get(self.base_url, params=params)
                data = response.json()
                
                quote = data.get("Global Quote", {})
                return {
                    "symbol": quote.get("01. symbol"),
                    "price": float(quote.get("05. price", 0)),
                    "change": float(quote.get("09. change", 0)),
                    "change_percent": quote.get("10. change percent", "").rstrip("%"),
                    "volume": int(quote.get("06. volume", 0)),
                    "latest_trading_day": quote.get("07. latest trading day"),
                    "timestamp": datetime.now().isoformat()
                }
    
    async def get_multiple_indicators(self, symbol: str, indicators: List[str]) -> dict:
        """å¹¶è¡Œè·å–å¤šä¸ªæŠ€æœ¯æŒ‡æ ‡"""
        
        # æ„å»ºå¹¶è¡Œä»»åŠ¡
        tasks = []
        for indicator in indicators:
            task = self._get_single_indicator(symbol, indicator)
            tasks.append(task)
        
        # å¹¶è¡Œæ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ•´åˆç»“æœ
        indicator_data = {}
        for i, indicator in enumerate(indicators):
            if not isinstance(results[i], Exception):
                indicator_data[indicator.lower()] = results[i]
        
        return indicator_data
    
    async def get_company_overview(self, symbol: str) -> dict:
        """è·å–å…¬å¸åŸºæœ¬é¢æ•°æ®"""
        async with self.rate_limiter:
            params = {
                "function": "OVERVIEW",
                "symbol": symbol,
                "apikey": self.api_key
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.get(self.base_url, params=params)
                data = response.json()
                
                return {
                    "symbol": data.get("Symbol"),
                    "name": data.get("Name"),
                    "description": data.get("Description"),
                    "sector": data.get("Sector"),
                    "industry": data.get("Industry"),
                    "market_cap": data.get("MarketCapitalization"),
                    "pe_ratio": data.get("PERatio"),
                    "peg_ratio": data.get("PEGRatio"),
                    "book_value": data.get("BookValue"),
                    "dividend_yield": data.get("DividendYield"),
                    "beta": data.get("Beta"),
                    "52_week_high": data.get("52WeekHigh"),
                    "52_week_low": data.get("52WeekLow")
                }
```

### 4.7 åŒé“¾è·¯ç›‘æ§ä¸ç®¡ç†
```python
class LinkageMonitor:
    """åŒé“¾è·¯ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            "direct_link": {
                "total_requests": 0,
                "cache_hits": 0,
                "avg_response_time": 0,
                "error_rate": 0,
                "status": "healthy"
            },
            "agent_link": {
                "total_requests": 0,
                "completed_tasks": 0,
                "avg_processing_time": 0,
                "error_rate": 0,
                "status": "healthy"
            }
        }
    
    def record_direct_request(self, response_time: float, cache_hit: bool, success: bool):
        """è®°å½•ç›´æ¥é“¾è·¯è¯·æ±‚"""
        metrics = self.metrics["direct_link"]
        metrics["total_requests"] += 1
        
        if cache_hit:
            metrics["cache_hits"] += 1
        
        if success:
            # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
            total_time = metrics["avg_response_time"] * (metrics["total_requests"] - 1)
            metrics["avg_response_time"] = (total_time + response_time) / metrics["total_requests"]
        else:
            # æ›´æ–°é”™è¯¯ç‡
            errors = metrics["error_rate"] * (metrics["total_requests"] - 1) + 1
            metrics["error_rate"] = errors / metrics["total_requests"]
    
    def record_agent_request(self, processing_time: float, completed: bool, success: bool):
        """è®°å½•Agenté“¾è·¯è¯·æ±‚"""
        metrics = self.metrics["agent_link"]
        metrics["total_requests"] += 1
        
        if completed:
            metrics["completed_tasks"] += 1
            
            # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
            total_time = metrics["avg_processing_time"] * (metrics["completed_tasks"] - 1)
            metrics["avg_processing_time"] = (total_time + processing_time) / metrics["completed_tasks"]
        
        if not success:
            # æ›´æ–°é”™è¯¯ç‡
            errors = metrics["error_rate"] * (metrics["total_requests"] - 1) + 1
            metrics["error_rate"] = errors / metrics["total_requests"]
    
    def get_system_status(self) -> dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        
        # è¯„ä¼°ç›´æ¥é“¾è·¯å¥åº·çŠ¶æ€
        direct_metrics = self.metrics["direct_link"]
        if direct_metrics["error_rate"] > 0.1:  # é”™è¯¯ç‡è¶…è¿‡10%
            direct_metrics["status"] = "unhealthy"
        elif direct_metrics["avg_response_time"] > 0.5:  # å¹³å‡å“åº”æ—¶é—´è¶…è¿‡500ms
            direct_metrics["status"] = "degraded"
        else:
            direct_metrics["status"] = "healthy"
        
        # è¯„ä¼°Agenté“¾è·¯å¥åº·çŠ¶æ€
        agent_metrics = self.metrics["agent_link"]
        if agent_metrics["error_rate"] > 0.15:  # é”™è¯¯ç‡è¶…è¿‡15%
            agent_metrics["status"] = "unhealthy"
        elif agent_metrics["avg_processing_time"] > 30:  # å¹³å‡å¤„ç†æ—¶é—´è¶…è¿‡30ç§’
            agent_metrics["status"] = "degraded"
        else:
            agent_metrics["status"] = "healthy"
        
        return {
            "timestamp": datetime.now().isoformat(),
            "direct_link": direct_metrics,
            "agent_link": agent_metrics,
            "overall_status": "healthy" if all(m["status"] == "healthy" for m in self.metrics.values()) else "degraded"
        }
```

## 10. éƒ¨ç½²é…ç½®

### 10.1 Dockeré…ç½®
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# ç¯å¢ƒå˜é‡
ENV PYTHONUNBUFFERED=1
ENV REDIS_URL=redis://redis:6379
ENV ALPHA_VANTAGE_API_KEY=${ALPHA_VANTAGE_API_KEY}

# è¿è¡Œåº”ç”¨
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 10.2 åŒé“¾è·¯éƒ¨ç½²æ¶æ„
```yaml
# docker-compose.yml - åŒé“¾è·¯éƒ¨ç½²
version: '3.8'

services:
  # åç«¯æœåŠ¡
  stock-backend:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ALPHA_VANTAGE_API_KEY=${ALPHA_VANTAGE_API_KEY}
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - redis
      - langgraph-service
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # LangGraph AgentæœåŠ¡
  langgraph-service:
    build: ./langgraph
    ports:
      - "8001:8001"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ALPHA_VANTAGE_API_KEY=${ALPHA_VANTAGE_API_KEY}
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.3'

  # ç›‘æ§æœåŠ¡
  monitor:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    restart: unless-stopped

volumes:
  redis_data:
```

### 10.3 æ€§èƒ½è°ƒä¼˜é…ç½®
```python
# config.py - åŒé“¾è·¯æ€§èƒ½é…ç½®

class DirectLinkConfig:
    """ç›´æ¥é“¾è·¯é…ç½®"""
    
    # ç¼“å­˜é…ç½®
    CACHE_TTL = {
        'real_time_data': 60,      # å®æ—¶æ•°æ® - 1åˆ†é’Ÿ
        'technical_indicators': 300,  # æŠ€æœ¯æŒ‡æ ‡ - 5åˆ†é’Ÿ
        'company_info': 3600,      # å…¬å¸ä¿¡æ¯ - 1å°æ—¶
    }
    
    # è¶…æ—¶é…ç½®
    TIMEOUTS = {
        'alpha_vantage_api': 5,    # Alpha Vantage APIè¶…æ—¶
        'redis_operation': 2,      # Redisæ“ä½œè¶…æ—¶
        'total_response': 10,      # æ€»å“åº”è¶…æ—¶
    }
    
    # ç†”æ–­å™¨é…ç½®
    CIRCUIT_BREAKER = {
        'failure_threshold': 5,     # å¤±è´¥é˜ˆå€¼
        'recovery_timeout': 60,   # æ¢å¤è¶…æ—¶æ—¶é—´
        'expected_exception': Exception,
    }
    
    # è¿æ¥æ± é…ç½®
    CONNECTION_POOL = {
        'max_connections': 100,
        'max_keepalive_connections': 20,
        'keepalive_expiry': 30,
    }

class AgentLinkConfig:
    """Agenté“¾è·¯é…ç½®"""
    
    # LangGraphé…ç½®
    LANGGRAPH = {
        'max_iterations': 10,     # æœ€å¤§è¿­ä»£æ¬¡æ•°
        'timeout_seconds': 300,   # æ€»è¶…æ—¶æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰
        'memory_limit_mb': 512,   # å†…å­˜é™åˆ¶
    }
    
    # LLMé…ç½®
    LLM = {
        'model': 'gpt-4-turbo-preview',
        'temperature': 0.1,        # ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
        'max_tokens': 4000,      # æœ€å¤§tokenæ•°
        'timeout': 60,           # LLMè°ƒç”¨è¶…æ—¶
    }
    
    # ä»»åŠ¡é˜Ÿåˆ—é…ç½®
    TASK_QUEUE = {
        'max_concurrent_tasks': 10,     # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
        'task_timeout': 900,           # ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆ15åˆ†é’Ÿï¼‰
        'retry_attempts': 2,           # é‡è¯•æ¬¡æ•°
        'retry_delay': 5,              # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    }
    
    # å†…å­˜ç®¡ç†
    MEMORY = {
        'max_session_memory_mb': 256,  # å•ä¸ªä¼šè¯å†…å­˜é™åˆ¶
        'cleanup_interval': 300,       # æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
        'max_session_duration': 3600,  # æœ€å¤§ä¼šè¯æ—¶é•¿ï¼ˆ1å°æ—¶ï¼‰
    }

class SystemConfig:
    """ç³»ç»Ÿçº§é…ç½®"""
    
    # ç›‘æ§é…ç½®
    MONITORING = {
        'metrics_collection_interval': 10,  # æŒ‡æ ‡æ”¶é›†é—´éš”ï¼ˆç§’ï¼‰
        'alert_threshold': {
            'error_rate': 0.1,             # é”™è¯¯ç‡å‘Šè­¦é˜ˆå€¼
            'response_time': 1.0,          # å“åº”æ—¶é—´å‘Šè­¦é˜ˆå€¼ï¼ˆç§’ï¼‰
            'queue_length': 50,            # é˜Ÿåˆ—é•¿åº¦å‘Šè­¦é˜ˆå€¼
        },
        'retention_days': 7,               # æ•°æ®ä¿ç•™å¤©æ•°
    }
    
    # æ—¥å¿—é…ç½®
    LOGGING = {
        'level': 'INFO',
        'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        'max_file_size_mb': 100,
        'backup_count': 10,
    }
    
    # å®‰å…¨é…ç½®
    SECURITY = {
        'rate_limit_per_minute': 60,       # æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶
        'api_key_rotation_days': 90,       # APIå¯†é’¥è½®æ¢å¤©æ•°
        'encryption_key_expiry_days': 365, # åŠ å¯†å¯†é’¥è¿‡æœŸå¤©æ•°
    }
```
### 10.4 ç¯å¢ƒå˜é‡é…ç½®
```bash
# ===== Alpha Vantage APIé…ç½® =====
ALPHA_VANTAGE_API_KEY=your_api_key_here
ALPHA_VANTAGE_BASE_URL=https://www.alphavantage.co/query
API_RATE_LIMIT=5  # æ¯åˆ†é’Ÿè°ƒç”¨æ¬¡æ•°
API_TIMEOUT=30    # APIè°ƒç”¨è¶…æ—¶ï¼ˆç§’ï¼‰

# ===== Redisé…ç½® =====
REDIS_URL=redis://localhost:6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_SSL=false

# ===== ç›´æ¥é“¾è·¯é…ç½® =====
DIRECT_LINK_CACHE_TTL=60           # ç¼“å­˜TTLï¼ˆç§’ï¼‰
DIRECT_LINK_TIMEOUT=10             # å“åº”è¶…æ—¶ï¼ˆç§’ï¼‰
DIRECT_LINK_MAX_RETRY=3            # æœ€å¤§é‡è¯•æ¬¡æ•°
DIRECT_LINK_CIRCUIT_BREAKER_THRESHOLD=5  # ç†”æ–­å™¨é˜ˆå€¼

# ===== Agenté“¾è·¯é…ç½® =====
LANGGRAPH_TIMEOUT=300              # LangGraphè¶…æ—¶ï¼ˆç§’ï¼‰
LANGGRAPH_MAX_ITERATIONS=10        # æœ€å¤§è¿­ä»£æ¬¡æ•°
LLM_MODEL=gpt-4-turbo-preview      # LLMæ¨¡å‹
LLM_TEMPERATURE=0.1                # LLMæ¸©åº¦å‚æ•°
LLM_MAX_TOKENS=4000                # æœ€å¤§tokenæ•°
AGENT_MAX_CONCURRENT_TASKS=10     # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
AGENT_TASK_TIMEOUT=900             # ä»»åŠ¡è¶…æ—¶ï¼ˆç§’ï¼‰

# ===== AIæœåŠ¡é…ç½® =====
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL=https://api.openai.com/v1
ANTHROPIC_API_KEY=your_anthropic_key
ANTHROPIC_BASE_URL=https://api.anthropic.com

# ===== ç›‘æ§é…ç½® =====
MONITORING_ENABLED=true
METRICS_COLLECTION_INTERVAL=10     # æŒ‡æ ‡æ”¶é›†é—´éš”ï¼ˆç§’ï¼‰
ALERT_ERROR_RATE_THRESHOLD=0.1     # é”™è¯¯ç‡å‘Šè­¦é˜ˆå€¼
ALERT_RESPONSE_TIME_THRESHOLD=1.0   # å“åº”æ—¶é—´å‘Šè­¦é˜ˆå€¼ï¼ˆç§’ï¼‰

# ===== æ—¥å¿—é…ç½® =====
LOG_LEVEL=INFO
LOG_FORMAT=json                      # æ—¥å¿—æ ¼å¼ï¼ˆtext/jsonï¼‰
LOG_MAX_FILE_SIZE_MB=100            # æ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°ï¼ˆMBï¼‰
LOG_BACKUP_COUNT=10                 # æ—¥å¿—å¤‡ä»½æ•°é‡

# ===== å®‰å…¨é…ç½® =====
RATE_LIMIT_PER_MINUTE=60            # æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶
API_KEY_ROTATION_DAYS=90            # APIå¯†é’¥è½®æ¢å¤©æ•°
ENCRYPTION_KEY_EXPIRY_DAYS=365    # åŠ å¯†å¯†é’¥è¿‡æœŸå¤©æ•°
CORS_ORIGINS=*                      # CORSå…è®¸æº

# ===== éƒ¨ç½²é…ç½® =====
ENVIRONMENT=production              # ç¯å¢ƒï¼ˆdevelopment/staging/productionï¼‰
DEBUG=false                         # è°ƒè¯•æ¨¡å¼
MAX_WORKERS=4                       # æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
PORT=8000                          # æœåŠ¡ç«¯å£
```

### 10.5 å¯åŠ¨è„šæœ¬
```bash
#!/bin/bash
# start.sh - åŒé“¾è·¯å¯åŠ¨è„šæœ¬

echo "ğŸš€ å¯åŠ¨è‚¡ç¥¨åˆ†æåŒé“¾è·¯ç³»ç»Ÿ..."

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if [ -z "$ALPHA_VANTAGE_API_KEY" ]; then
    echo "âŒ é”™è¯¯ï¼šALPHA_VANTAGE_API_KEY æœªè®¾ç½®"
    exit 1
fi

if [ -z "$OPENAI_API_KEY" ]; then
    echo "âŒ é”™è¯¯ï¼šOPENAI_API_KEY æœªè®¾ç½®"
    exit 1
fi

# å¯åŠ¨Redis
echo "ğŸ“¦ å¯åŠ¨Redisç¼“å­˜æœåŠ¡..."
docker-compose up -d redis

# ç­‰å¾…Rediså¯åŠ¨
sleep 5

# å¯åŠ¨LangGraphæœåŠ¡
echo "ğŸ§  å¯åŠ¨LangGraph AgentæœåŠ¡..."
docker-compose up -d langgraph-service

# ç­‰å¾…LangGraphæœåŠ¡å¯åŠ¨
sleep 10

# å¯åŠ¨åç«¯æœåŠ¡
echo "âš¡ å¯åŠ¨Pythonåç«¯æœåŠ¡..."
docker-compose up -d stock-backend

# å¯åŠ¨ç›‘æ§æœåŠ¡
echo "ğŸ“Š å¯åŠ¨ç›‘æ§æœåŠ¡..."
docker-compose up -d monitor

echo "âœ… åŒé“¾è·¯ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼"
echo ""
echo "ğŸŒ æœåŠ¡åœ°å€ï¼š"
echo "   - åç«¯API: http://localhost:8000"
echo "   - LangGraph: http://localhost:8001"
echo "   - ç›‘æ§é¢æ¿: http://localhost:9090"
echo ""
echo "ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ï¼š"
echo "   - ç›´æ¥é“¾è·¯å“åº”æ—¶é—´: < 200ms"
echo "   - Agenté“¾è·¯å¤„ç†æ—¶é—´: 5-15ç§’"
echo "   - ç³»ç»Ÿå¹¶å‘èƒ½åŠ›: 1000+ QPS"
```

è¿™ä¸ªçº¯APIé©±åŠ¨çš„æ¶æ„è®¾è®¡å®Œå…¨ç§»é™¤äº†æ•°æ®åº“ä¾èµ–ï¼Œæ‰€æœ‰æ•°æ®éƒ½é€šè¿‡Alpha Vantage APIå®æ—¶è·å–ï¼Œä½¿ç”¨Redisè¿›è¡ŒçŸ­æœŸç¼“å­˜ï¼Œæ”¯æŒç›´æ¥APIè°ƒç”¨å’Œæ™ºèƒ½Agentåˆ†æä¸¤ç§æ¨¡å¼ï¼Œæä¾›äº†é«˜å¯ç”¨æ€§å’Œæ‰©å±•æ€§ã€‚